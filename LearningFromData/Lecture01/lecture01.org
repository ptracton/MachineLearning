* Lecture 01 The Learning Problem
- These are my notes based on the online course [[https://work.caltech.edu/telecourse.html][Learning from Data]] at Cal Tech by Professor Yaser Abu-Mostafa
- The first lecture is at [[https://www.youtube.com/watch?v=mbyG85GZ0PI&hd=1][You Tube]]
- The audio does kind of fade in and out

** Story Line
There is a theme through the course.  Each lecture is not wholely independent from each other.  The key steps in our story are
- What is Learning?
- Can we learn?
- How to learn?
- How to learn well?
- Take home lessons
Lecture 3 is a practical topic not really part of the story.  Gives you tools to actually work on this material.  Avoids the course from being too theoretical at the start.

** Examples of Machine Learning
- Predicting how a viewer will rate a movie
- The essesnce of machine learning, need these 3 components to do machine learning
  - A pattern exists → without there is nothing to look for
  - We can not pin it down mathematically → can not write down a single equation for this
  - We have to have data → no data no learning

*** Learning Approach
- Each viewer's vector will be different
- Each movie's vector will be different
- combine these 2 to see if a user will like a particular movie
- Machine Learning will reverse engineer this process
  - start with rating and find consistent factors
  - Nudge factors of the vectors to get back to rating ever so slowly
  - Do this not for a single rating but millions.  Do it over and over again and eventually the factors become mearningful for the ratings.

** Components of Learning
- Credit Card example
  - bank wants to make money on new cards
  - based on historical data predict how a new customer will do

- Formalization
  - Input X (customer application)
  - Output Y (give credit or no)
  - Target function f: x → y (ideal credit approval formula)
  - Data (x_1, y_1), (x_2, y_2) ... (x_N, y_N) (historical record)
  - Target function in machine learning is the unknown, solve with data
  - Hypothesis is the formual that approximates the target function g: x → y (forumla to be used)
    - g approximates f.  g is known f is not!
  - Data used to train the learning algorithm to make g approximate f
  - Learning Algorithm based on data and Hypothesis Set of forumlas \textcolor{red}{(where do these come from?)}  Guesses at g, learning algorithm will pick the winner.
  - Why have hypothesis set?
    - There is no downside to it you decide how you are learning (linear, neural net, etc...)
    - There is an upside not obvious now.  Plays a pivotal role.  Lets us know if we can learn.
    - You can do a set of all possible hypothesis

- Solution Components
  - 2 solutions components to learning
    - No control over target function
    - No control over data
    - Final hyposthesis is dictated
    - Learning Algorithm and Hypothesis set are your solution tools!
  - Hypothesis Set.  The small h is the function the large one is all of the possible options.  g is the selected one.
$$
\mathscr{H} = {h}
$$
$$
g \in \mathscr{H}
$$
  - The Learning Algorithm and Hypothesis together are the \textcolor{red}{Learning Model}.  Many options.

** A simple model
- Simple hypothesis the perception, very simple and not very useful in reality
- For input x = (x_1, x_2, ... x_d) are attributes of the customer
- The w vector is weighing which of the inputs x are important.  
- approve credit if \sum_{i=1}^{d} w_{i}*x_{i} > threshold
- deny if below threshold
- this is sort of a credit score
- do not know the w vector or threshold
- The linear formula h \in \mathscr{H}
$$
h(x) = sign((\sum_{i=1}^{d} w_{i}*x_{i}) - threshold)
$$
- Change notation and consider threshold as a weight (w_{0} = - threshold)
$$
h(x) = sign((\sum_{i=1}^{d} w_{i}*x_{i}) + w_{0})
$$
- Introduce artificial coordinate x_0 = 1 to simplify the equation to 
$$
h(x) = sign(\sum_{i=0}^{d} w_{i}*x_{i})
$$
- In vector form, inner product of column w and vector x
$$
h(x) = sign(\textbf{w}^{T}\textbf{x})
$$

*** Perceptron Learning Algorithm
- Implements
$$
h(x) = sign(\textbf{w}^{T}\textbf{x})
$$
- uses historical data in attempt to make w correct
- pick a \textcolor{red}{misclassified point}
$$
sign(\textbf{w}^{T}\textbf{x}) \neq y_{n}
$$

- Update the weight (w) vector to be better for this point, y_n is +1 or -1

$$
\textcolor{blue}{\textbf{w}} \leftarrow \textcolor{red}{\textbf{w}} + y_{n}*x_{n}
$$

**** Iterations of PLA
- One iteration, where (x,y) is misclassified
$$
\textcolor{blue}{\textbf{w}} \leftarrow \textcolor{red}{\textbf{w}} + y*x
$$

- At iteration t=1,2,3... pick a misclassified point from (x_1, y_1), (x_2, y_2) ... (x_N, y_N) and run a PLA iteration on it
- That's it!
- Here is an implementation [[https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/][from machine learning master]]

** Types of Learning
*** Basic Premise of Learning
- using a set of observations to uncover and underlying process
- very broad, leads to many variations
- Types
  - Supervised Learning -- concentation of this course
    - Any time the data and output are explicitly given, like a supervisor is helping you out
  - Unsupervised learning 
    - we get input data and no outputs.  Like listening to another language on radio in an effort to learning it
  - Reinforcement Learning
    - get the input data and \textit{some} of the output and grade of your output.  Great for playing games

** Puzzle
- Superivised Learning puzzle
  - I guess -1
  - Doesn't matter, wants to get both answers and impossible to answer this particular problem
** Q & A
*** How to determine if linear seperable?
  - Rarely true, good for examples
  - Techniques to make it true
  - Assume this is false
  - pocket algoritm?
*** How do you know if there is a pattern?
    - You don't
    - Covered in a future lecture
    - Take data, apply algorithm and you can detect if you learn or not and knowing this
    - avoid looking at data
*** Global optimization or local optimization?
    - Whichever works for us
*** Hypothesis continuous or discrete
    - Can be either
*** How much data for a particular problem?
    - Theory: this is the crux of theory
    - Practical: not under your control
